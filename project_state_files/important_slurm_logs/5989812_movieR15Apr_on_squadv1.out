04/25/2021 20:39:08 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
04/25/2021 20:39:08 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/scratch/as11919/Domain-Adaptation/models/movieR_15April2021_on_squadv1, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.STEPS, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=20, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Apr25_20-39-08_gv03.nyu.cluster, logging_strategy=IntervalStrategy.STEPS, logging_first_step=True, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=7500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=7500, dataloader_num_workers=0, past_index=-1, run_name=MovieR-15Apr21, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard', 'wandb'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, _n_gpu=1, mp_parameters=)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 6.0107, 'learning_rate': 4.99981936416185e-05, 'epoch': 0.0}
{'loss': 1.8584, 'learning_rate': 4.909682080924856e-05, 'epoch': 0.18}
{'loss': 1.2301, 'learning_rate': 4.8193641618497106e-05, 'epoch': 0.36}
{'loss': 1.1372, 'learning_rate': 4.729046242774567e-05, 'epoch': 0.54}
{'loss': 1.0786, 'learning_rate': 4.6387283236994224e-05, 'epoch': 0.72}
{'loss': 1.0322, 'learning_rate': 4.548410404624278e-05, 'epoch': 0.9}
{'loss': 0.8981, 'learning_rate': 4.458092485549133e-05, 'epoch': 1.08}
{'loss': 0.7827, 'learning_rate': 4.367774566473989e-05, 'epoch': 1.26}
{'loss': 0.7838, 'learning_rate': 4.2774566473988445e-05, 'epoch': 1.45}
{'loss': 0.7882, 'learning_rate': 4.1871387283236994e-05, 'epoch': 1.63}
{'loss': 0.7835, 'learning_rate': 4.096820809248555e-05, 'epoch': 1.81}
{'loss': 0.7752, 'learning_rate': 4.006502890173411e-05, 'epoch': 1.99}
{'loss': 0.5687, 'learning_rate': 3.916184971098266e-05, 'epoch': 2.17}
{'loss': 0.5823, 'learning_rate': 3.8258670520231215e-05, 'epoch': 2.35}
{'loss': 0.582, 'learning_rate': 3.735549132947977e-05, 'epoch': 2.53}
{'loss': 0.5866, 'learning_rate': 3.6452312138728326e-05, 'epoch': 2.71}

04/25/2021 22:19:43 - INFO - utils_qa -   Post-processing 10570 example predictions split into 10790 features.
04/25/2021 22:20:03 - INFO - utils_qa -   Saving predictions to /scratch/as11919/Domain-Adaptation/models/movieR_15April2021_on_squadv1/eval_predictions.json.
04/25/2021 22:20:03 - INFO - utils_qa -   Saving nbest_preds to /scratch/as11919/Domain-Adaptation/models/movieR_15April2021_on_squadv1/eval_nbest_predictions.json.
{'exact_match': 82.14758751182592, 'f1': 89.33944539556455, 'epoch': 2.71}
{'loss': 0.5997, 'learning_rate': 3.554913294797688e-05, 'epoch': 2.89}
{'loss': 0.5314, 'learning_rate': 3.464595375722544e-05, 'epoch': 3.07}
{'loss': 0.436, 'learning_rate': 3.374277456647399e-05, 'epoch': 3.25}
{'loss': 0.4437, 'learning_rate': 3.283959537572254e-05, 'epoch': 3.43}
{'loss': 0.4462, 'learning_rate': 3.19364161849711e-05, 'epoch': 3.61}
{'loss': 0.4393, 'learning_rate': 3.103323699421966e-05, 'epoch': 3.79}
{'loss': 0.4578, 'learning_rate': 3.013005780346821e-05, 'epoch': 3.97}
{'loss': 0.3365, 'learning_rate': 2.9226878612716762e-05, 'epoch': 4.15}
{'loss': 0.3264, 'learning_rate': 2.832369942196532e-05, 'epoch': 4.34}
{'loss': 0.3322, 'learning_rate': 2.7420520231213876e-05, 'epoch': 4.52}
{'loss': 0.3355, 'learning_rate': 2.651734104046243e-05, 'epoch': 4.7}
{'loss': 0.338, 'learning_rate': 2.5614161849710984e-05, 'epoch': 4.88}
{'loss': 0.3046, 'learning_rate': 2.471098265895954e-05, 'epoch': 5.06}
{'loss': 0.2471, 'learning_rate': 2.380780346820809e-05, 'epoch': 5.24}
{'loss': 0.2411, 'learning_rate': 2.290462427745665e-05, 'epoch': 5.42}
04/25/2021 23:59:51 - INFO - utils_qa -   Post-processing 10570 example predictions split into 10790 features.
04/26/2021 00:00:11 - INFO - utils_qa -   Saving predictions to /scratch/as11919/Domain-Adaptation/models/movieR_15April2021_on_squadv1/eval_predictions.json.
04/26/2021 00:00:11 - INFO - utils_qa -   Saving nbest_preds to /scratch/as11919/Domain-Adaptation/models/movieR_15April2021_on_squadv1/eval_nbest_predictions.json.
{'exact_match': 81.20151371807, 'f1': 89.17851833774992, 'epoch': 5.42}
{'loss': 0.2455, 'learning_rate': 2.2001445086705202e-05, 'epoch': 5.6}
{'loss': 0.2452, 'learning_rate': 2.1098265895953757e-05, 'epoch': 5.78}
{'loss': 0.2507, 'learning_rate': 2.0195086705202312e-05, 'epoch': 5.96}
{'loss': 0.1927, 'learning_rate': 1.9291907514450868e-05, 'epoch': 6.14}
{'loss': 0.1814, 'learning_rate': 1.8388728323699423e-05, 'epoch': 6.32}
{'loss': 0.1847, 'learning_rate': 1.748554913294798e-05, 'epoch': 6.5}
{'loss': 0.189, 'learning_rate': 1.6582369942196534e-05, 'epoch': 6.68}
{'loss': 0.1885, 'learning_rate': 1.567919075144509e-05, 'epoch': 6.86}
{'loss': 0.1707, 'learning_rate': 1.4776011560693643e-05, 'epoch': 7.04}
{'loss': 0.1278, 'learning_rate': 1.3872832369942197e-05, 'epoch': 7.23}
{'loss': 0.1325, 'learning_rate': 1.2969653179190752e-05, 'epoch': 7.41}
{'loss': 0.1389, 'learning_rate': 1.2066473988439307e-05, 'epoch': 7.59}
{'loss': 0.129, 'learning_rate': 1.1163294797687863e-05, 'epoch': 7.77}
{'loss': 0.1309, 'learning_rate': 1.0260115606936416e-05, 'epoch': 7.95}
{'loss': 0.1136, 'learning_rate': 9.356936416184972e-06, 'epoch': 8.13}
04/26/2021 01:40:00 - INFO - utils_qa -   Post-processing 10570 example predictions split into 10790 features.
04/26/2021 01:40:19 - INFO - utils_qa -   Saving predictions to /scratch/as11919/Domain-Adaptation/models/movieR_15April2021_on_squadv1/eval_predictions.json.
04/26/2021 01:40:19 - INFO - utils_qa -   Saving nbest_preds to /scratch/as11919/Domain-Adaptation/models/movieR_15April2021_on_squadv1/eval_nbest_predictions.json.
{'exact_match': 80.70955534531693, 'f1': 88.92918416422384, 'epoch': 8.13}
{'loss': 0.1045, 'learning_rate': 8.453757225433527e-06, 'epoch': 8.31}
{'loss': 0.1034, 'learning_rate': 7.550578034682081e-06, 'epoch': 8.49}
{'loss': 0.0969, 'learning_rate': 6.647398843930635e-06, 'epoch': 8.67}
{'loss': 0.0981, 'learning_rate': 5.7442196531791915e-06, 'epoch': 8.85}
{'loss': 0.0942, 'learning_rate': 4.841040462427745e-06, 'epoch': 9.03}
{'loss': 0.0797, 'learning_rate': 3.9378612716763006e-06, 'epoch': 9.21}
{'loss': 0.0814, 'learning_rate': 3.0346820809248555e-06, 'epoch': 9.39}
{'loss': 0.0759, 'learning_rate': 2.1315028901734105e-06, 'epoch': 9.57}
{'loss': 0.0666, 'learning_rate': 1.2283236994219654e-06, 'epoch': 9.75}
{'loss': 0.076, 'learning_rate': 3.2514450867052023e-07, 'epoch': 9.93}
{'train_runtime': 22100.7751, 'train_samples_per_second': 1.252, 'epoch': 10.0}
04/26/2021 02:48:17 - INFO - __main__ -   *** Evaluate ***
04/26/2021 02:49:46 - INFO - utils_qa -   Post-processing 10570 example predictions split into 10790 features.
04/26/2021 02:50:06 - INFO - utils_qa -   Saving predictions to /scratch/as11919/Domain-Adaptation/models/movieR_15April2021_on_squadv1/eval_predictions.json.
04/26/2021 02:50:06 - INFO - utils_qa -   Saving nbest_preds to /scratch/as11919/Domain-Adaptation/models/movieR_15April2021_on_squadv1/eval_nbest_predictions.json.

