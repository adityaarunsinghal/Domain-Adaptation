/ext3/miniconda3/lib/python3.8/site-packages/parlai/utils/fp16.py:487: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
Traceback (most recent call last):
  File "/ext3/miniconda3/bin/parlai", line 8, in <module>
    sys.exit(main())
  File "/ext3/miniconda3/lib/python3.8/site-packages/parlai/__main__.py", line 14, in main
    superscript_main()
  File "/ext3/miniconda3/lib/python3.8/site-packages/parlai/core/script.py", line 306, in superscript_main
    return SCRIPT_REGISTRY[cmd].klass._run_from_parser_and_opt(opt, parser)
  File "/ext3/miniconda3/lib/python3.8/site-packages/parlai/core/script.py", line 89, in _run_from_parser_and_opt
    return script.run()
  File "/ext3/miniconda3/lib/python3.8/site-packages/parlai/scripts/train_model.py", line 789, in run
    return self.train_loop.train()
  File "/ext3/miniconda3/lib/python3.8/site-packages/parlai/scripts/train_model.py", line 679, in train
    world.parley()
  File "/ext3/miniconda3/lib/python3.8/site-packages/parlai/core/worlds.py", line 849, in parley
    batch_act = self.batch_act(agent_idx, batch_observations[agent_idx])
  File "/ext3/miniconda3/lib/python3.8/site-packages/parlai/core/worlds.py", line 817, in batch_act
    batch_actions = a.batch_act(batch_observation)
  File "/ext3/miniconda3/lib/python3.8/site-packages/parlai/core/torch_agent.py", line 2006, in batch_act
    output = self.train_step(batch)
  File "/ext3/miniconda3/lib/python3.8/site-packages/parlai/core/torch_generator_agent.py", line 750, in train_step
    self.update_params()
  File "/ext3/miniconda3/lib/python3.8/site-packages/parlai/core/torch_agent.py", line 2121, in update_params
    grad_norm = self.optimizer.clip_master_grads(self.opt['gradient_clip'])
  File "/ext3/miniconda3/lib/python3.8/site-packages/parlai/utils/fp16.py", line 328, in clip_master_grads
    raise FloatingPointError(
FloatingPointError: Minimum loss scale reached (0.0001). Your loss is probably exploding. Try lowering the learning rate, using gradient clipping or increasing the batch size.
